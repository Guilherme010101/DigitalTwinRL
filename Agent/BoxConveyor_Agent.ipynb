{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cfb722f",
   "metadata": {},
   "source": [
    "# 1 - Importa bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6fa3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch.nn import Tanh, ELU, ReLU, Sigmoid, Softmax\n",
    "import websocket\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import random\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import stable_baselines3\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete\n",
    "from stable_baselines3 import PPO, DQN, A2C, SAC, TD3, DDPG\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList, ProgressBarCallback, TensorboardCallback, EvalCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from sb3_contrib import RecurrentPPO, TQC, QRDQN, MaskablePPO, TRPO, ARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e4626",
   "metadata": {},
   "source": [
    "# 2 - Cria funções de encoding/decoding em json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217f60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):            \n",
    "            return obj.tolist()\n",
    "        \n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "identifier ='BoxConveyor_manual'\n",
    "actions =[]\n",
    "message = ''\n",
    "\n",
    "\n",
    "def encode_json(identifier, actions):\n",
    "    data = {}\n",
    "    data['identifier'] = identifier\n",
    "    data['actions'] = actions\n",
    "    json_data = json.dumps(data, cls=NumpyEncoder)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12bf18",
   "metadata": {},
   "source": [
    "# 3 - Define função de criação do agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc4b64b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrillEnv(Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def _reset(self):\n",
    "        \n",
    "        self.reward = 0\n",
    "        self.action_name = ''\n",
    "        self.Can = False\n",
    "        self.SensorHandling = False\n",
    "        self.SensorRobot = False\n",
    "        self.ConveyorPosition = 0\n",
    "        actions = []\n",
    "        self.number_steps = 0\n",
    "        self.buffer_size = tamanho_buffer        \n",
    "        self.state_buffer = np.zeros((self.buffer_size * 4))\n",
    "        \n",
    "    def _obs(self):\n",
    "        obs= self.state_buffer                     \n",
    "        return obs\n",
    "        \n",
    "\n",
    "    def setprint(self, print):\n",
    "        self.print = print \n",
    "    def setprint2(self, print):\n",
    "        self.print2 = print\n",
    "    \n",
    "    def __init__(self):     \n",
    "        super(DrillEnv, self).__init__()\n",
    "        self.reset()\n",
    "\n",
    "        self.print = False\n",
    "        self.print2 = False\n",
    "        self.action_space = Discrete(3)       \n",
    "        self.observation_space = Box(low=0, high=1, shape=(1,self.buffer_size * 4), dtype=np.float32)\n",
    "\n",
    "\n",
    "    def buffer(self, Can,SensorHandling, SensorRobot, ConveyorPosition):\n",
    "        \n",
    "        self.state_buffer = np.roll(self.state_buffer,4)\n",
    "         \n",
    "        self.state_buffer[0] = Can \n",
    "        self.state_buffer[1] = SensorHandling \n",
    "        self.state_buffer[2] = SensorRobot\n",
    "        self.state_buffer[3] = ConveyorPosition \n",
    "\n",
    "    def step(self, action):\n",
    "        actions = action \n",
    "        self.number_steps += 1\n",
    "        mensagem = encode_json('BoxConveyor_manual',  [actions])\n",
    "        \n",
    "        ws.send(mensagem)\n",
    "        data =''\n",
    "        station_identifier = ''\n",
    "        while  station_identifier != 'BoxConveyor':\n",
    "            try:\n",
    "                data = json.loads(ws.recv())                \n",
    "                station_identifier = (data['identifier'])\n",
    "            except:\n",
    "                return\n",
    "        \n",
    "        self.reward = int(data['reward'])\n",
    "         \n",
    "        if bool(data['done']) == False:\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "               \n",
    "        self.Can = bool(data['states'][0])\n",
    "        self.SensorHandling = bool(data['states'][1])\n",
    "        self.SensorRobot = bool(data['states'][2])\n",
    "        self.ConveyorPosition = float(data['states'][3])\n",
    "    \n",
    "        self.buffer(self.Can, self.SensorHandling, self.SensorRobot,self.ConveyorPosition) \n",
    "\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        obs=self._obs()\n",
    "        \n",
    "        actions_dict = {\n",
    "                            0: {'Forward': False, 'Backward': False},\n",
    "                            1: {'Forward': True, 'Backward': False},\n",
    "                            2: {'Forward': False, 'Backward': True}\n",
    "                        }\n",
    "        \n",
    "        \n",
    "        if self.print:  \n",
    "            print((self.number_steps),'recompensa: ',(self.reward),'|  acao --> ', actions_dict[actions],'  done: ', done)\n",
    "            for name, value in zip(['Can', 'SensorHandling', 'SensorRobot', 'ConveyorPosition'], self.state_buffer[0:4]):\n",
    "                print(f\"{name}: {value}\")\n",
    "            print('\\n')\n",
    "   \n",
    "         # Return step information\n",
    "        return obs, self.reward, done, info\n",
    "    \n",
    "    def render(self , mode):\n",
    "        pass\n",
    "    def reset(self):        \n",
    "        self._reset()\n",
    "        return self._obs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d2da0",
   "metadata": {},
   "source": [
    "# 4 - Cria o modelo em Aprendizagem por Reforço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddc52410",
   "metadata": {},
   "outputs": [],
   "source": [
    "algoritmo = PPO\n",
    "nome_do_ficheiro = \"PPO\"\n",
    "tamanho_buffer = 6 # tamanho do buffer aplicado ao algoritmo\n",
    "funcao_ativacao = Tanh #ex: Tanh, ELU, ReLU, Sigmoid, Softmax\n",
    "rede_neuronal=[dict(pi=[60,60], vf=[60,60])] # define número de nurónios na rede neuronal de política e rede neuronal de valor\n",
    "tipo_camadas = 'MlpPolicy' # define o tipo de camadas do modelo !!---(Para modelo em PPORecurrent, usar MlpLstmPolicy)---!!\n",
    "fator_desconto= 0.95 #fator de desconto para aprendizagem por reforço\n",
    "save_path = os.path.join('Training_BoxConveyor','Model_saves',f\"\"\"{nome_do_ficheiro}_B{tamanho_buffer}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf9aa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (shared_net): Sequential()\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=24, out_features=60, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=24, out_features=60, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=60, out_features=3, bias=True)\n",
      "  (value_net): Linear(in_features=60, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "save_path = os.path.join('Training_BoxConveyor','Model_saves',f\"\"\"{nome_do_ficheiro}_B{tamanho_buffer}\"\"\")\n",
    "policy_kwargs = dict(activation_fn=funcao_ativacao,net_arch=rede_neuronal)\n",
    "log_path = os.path.join('Training_BoxConveyor','Logs',f\"\"\"{nome_do_ficheiro}_B{tamanho_buffer}\"\"\")\n",
    "\n",
    "env=DrillEnv()\n",
    "env.setprint(False) ### imprime os estados e ações\n",
    "env=DummyVecEnv([lambda: env])\n",
    "env.reset()\n",
    "\n",
    "\n",
    "model=algoritmo(tipo_camadas,env,verbose=1,\n",
    "        gamma=fator_desconto, gae_lambda=0.95,\n",
    "        seed=9,policy_kwargs=policy_kwargs, \n",
    "        tensorboard_log=log_path)\n",
    "\n",
    "\n",
    "print(model.policy)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1e886",
   "metadata": {},
   "source": [
    "# 5 - Define funções de callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a689d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=10000,\n",
    "  save_path=os.path.join('Training_BoxConveyor','Model_saves',f\"\"\"{nome_do_ficheiro}_B{tamanho_buffer}_checkpoint\"\"\"),\n",
    "  name_prefix=f\"\"\"{nome_do_ficheiro}_B{tamanho_buffer}_checkpoint\"\"\")\n",
    "\n",
    "\n",
    "eval_callback = EvalCallback(env, n_eval_episodes=3,\n",
    "                             best_model_save_path=os.path.join('Training_BoxConveyor','Model_saves',f\"\"\"{nome_do_ficheiro}_B{tamanho_buffer}_backup\"\"\",'Best_model'),\n",
    "                             log_path=log_path, eval_freq=4096,\n",
    "                             deterministic=False, render=False)\n",
    "callback = CallbackList([checkpoint_callback, ProgressBarCallback(), TensorboardCallback(), eval_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf4189",
   "metadata": {},
   "source": [
    "# 6 - Inicia treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fc7e271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training_BoxConveyor\\Logs\\PPO_B6\\PPO_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24399730407e4f33a6a694be3cb89f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">C:\\Python\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not \n",
       "wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other \n",
       "wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "C:\\Python\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not \n",
       "wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other \n",
       "wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=4096, episode_reward=162.00 +/- 229.10\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=4096, episode_reward=162.00 +/- 229.10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 35.00 +/- 17.68\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 35.00 +/- 17.68\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 35         |\n",
      "|    mean_reward          | 162        |\n",
      "| random_value            | 0.332      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00814675 |\n",
      "|    clip_fraction        | 0.0422     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.000107   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.36e+03   |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00736   |\n",
      "|    value_loss           | 5.38e+03   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=8192, episode_reward=485.00 +/- 345.52\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=8192, episode_reward=485.00 +/- 345.52\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 58.67 +/- 29.83\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 58.67 +/- 29.83\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 58.7        |\n",
      "|    mean_reward          | 485         |\n",
      "| random_value            | 0.574       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012053081 |\n",
      "|    clip_fraction        | 0.0874      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.00166     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.82e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 1.02e+04    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=12288, episode_reward=195.33 +/- 276.24\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=12288, episode_reward=195.33 +/- 276.24\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 46.33 +/- 19.22\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 46.33 +/- 19.22\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 46.3         |\n",
      "|    mean_reward          | 195          |\n",
      "| random_value            | 0.972        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051790373 |\n",
      "|    clip_fraction        | 0.0272       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.917       |\n",
      "|    explained_variance   | -0.000116    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.89e+03     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00471     |\n",
      "|    value_loss           | 2.66e+04     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=16384, episode_reward=524.00 +/- 491.76\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=16384, episode_reward=524.00 +/- 491.76\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 43.00 +/- 24.34\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 43.00 +/- 24.34\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 43           |\n",
      "|    mean_reward          | 524          |\n",
      "| random_value            | 0.0504       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035431832 |\n",
      "|    clip_fraction        | 0.0102       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.807       |\n",
      "|    explained_variance   | -4.47e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.75e+04     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00248     |\n",
      "|    value_loss           | 3.32e+04     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ws = websocket.WebSocket()\n",
    "\n",
    "ws.connect(\"ws://127.0.0.1:12000\")\n",
    "time.sleep(1)\n",
    "\n",
    "for i in range(1):\n",
    "    model.learn(total_timesteps=15000,log_interval=200,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac8bd0",
   "metadata": {},
   "source": [
    "# 7 - Salva o modelo (caso necessário)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe450c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964f894",
   "metadata": {},
   "source": [
    "# 8 - Carrega o modelo (caso necessário)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70b3cb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando ficheiro:  PPO no caminho:  Training_BoxConveyor\\Model_saves\\PPO_B6\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (shared_net): Sequential()\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=24, out_features=60, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=24, out_features=60, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=60, out_features=3, bias=True)\n",
      "  (value_net): Linear(in_features=60, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "nome_do_ficheiro_carregar = \"PPO\" ###\n",
    "algoritmo_carregar = PPO ### Algoritmo a carregar\n",
    "tamanho_buffer_carregar = 6\n",
    "\n",
    "####------------------------------------***----------------------------------------------------####\n",
    "\n",
    "tamanho_buffer = tamanho_buffer_carregar\n",
    "load_path = os.path.join('Training_BoxConveyor','Model_saves',f\"\"\"{nome_do_ficheiro_carregar}_B{tamanho_buffer_carregar}\"\"\")\n",
    "model = algoritmo_carregar.load(load_path)\n",
    "print(\"Carregando ficheiro: \",nome_do_ficheiro_carregar, \"no caminho: \", load_path)\n",
    "print(\"\\n\",\"*\" * 100,\"\\n\" )\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fffa0b4",
   "metadata": {},
   "source": [
    "# 9 - Utiliza modelo treinado no ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12c071c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 recompensa:  0 |  acao -->  {'Forward': True, 'Backward': False}   done:  False\n",
      "Can: 0.0\n",
      "SensorHandling: 0.0\n",
      "SensorRobot: 0.0\n",
      "ConveyorPosition: 0.0\n",
      "\n",
      "\n",
      "2 recompensa:  0 |  acao -->  {'Forward': True, 'Backward': False}   done:  False\n",
      "Can: 0.0\n",
      "SensorHandling: 0.0\n",
      "SensorRobot: 0.0\n",
      "ConveyorPosition: 0.15\n",
      "\n",
      "\n",
      "3 recompensa:  0 |  acao -->  {'Forward': True, 'Backward': False}   done:  False\n",
      "Can: 0.0\n",
      "SensorHandling: 0.0\n",
      "SensorRobot: 0.0\n",
      "ConveyorPosition: 0.31\n",
      "\n",
      "\n",
      "4 recompensa:  0 |  acao -->  {'Forward': True, 'Backward': False}   done:  False\n",
      "Can: 0.0\n",
      "SensorHandling: 0.0\n",
      "SensorRobot: 0.0\n",
      "ConveyorPosition: 0.46\n",
      "\n",
      "\n",
      "5 recompensa:  0 |  acao -->  {'Forward': True, 'Backward': False}   done:  False\n",
      "Can: 0.0\n",
      "SensorHandling: 1.0\n",
      "SensorRobot: 0.0\n",
      "ConveyorPosition: 0.61\n",
      "\n",
      "\n",
      "6 recompensa:  100 |  acao -->  {'Forward': False, 'Backward': False}   done:  False\n",
      "Can: 0.0\n",
      "SensorHandling: 1.0\n",
      "SensorRobot: 0.0\n",
      "ConveyorPosition: 0.62\n",
      "\n",
      "\n",
      "7 recompensa:  100 |  acao -->  {'Forward': False, 'Backward': False}   done:  False\n",
      "Can: 0.0\n",
      "SensorHandling: 1.0\n",
      "SensorRobot: 0.0\n",
      "ConveyorPosition: 0.62\n",
      "\n",
      "\n",
      "8 recompensa:  100 |  acao -->  {'Forward': False, 'Backward': False}   done:  False\n",
      "Can: 1.0\n",
      "SensorHandling: 1.0\n",
      "SensorRobot: 0.0\n",
      "ConveyorPosition: 0.62\n",
      "\n",
      "\n",
      "9 recompensa:  100 |  acao -->  {'Forward': False, 'Backward': False}   done:  False\n",
      "Can: 0.0\n",
      "SensorHandling: 1.0\n",
      "SensorRobot: 0.0\n",
      "ConveyorPosition: 0.62\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \n\u001b[0;32m     17\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 18\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     19\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender() \n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done: \n",
      "File \u001b[1;32mC:\\Python\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 43\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mstep(\n\u001b[0;32m     44\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[env_idx]\n\u001b[0;32m     45\u001b[0m         )\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "\n",
    "env=DrillEnv()\n",
    "env.setprint(True) ### Imprime estados e ações\n",
    "env=DummyVecEnv([lambda: env])\n",
    "env.reset()\n",
    "\n",
    "  \n",
    "ws = websocket.WebSocket()\n",
    "ws.connect(\"ws://127.0.0.1:12000\")\n",
    "time.sleep(1)\n",
    "mensagem = encode_json('CanConveyor_automatico', []) ### Para deixar a esteira de latas no modo automático\n",
    "ws.send(mensagem)\n",
    "mensagem = encode_json('Handling_automatico', []) ### Para deixar a garra de latas no modo automático\n",
    "time.sleep(1)\n",
    "ws.send(mensagem)\n",
    "obs = env.reset()\n",
    "while True: \n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render() \n",
    "    if done: \n",
    "        obs = env.reset()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba24a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
