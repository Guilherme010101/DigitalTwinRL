{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9117b888",
   "metadata": {},
   "source": [
    "# 1 - Importa bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab0f0694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch.nn import Tanh, ELU, ReLU, Sigmoid, Softmax\n",
    "import websocket\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import random\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import stable_baselines3\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete\n",
    "from stable_baselines3 import PPO, DQN, A2C, SAC, TD3, DDPG\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList, ProgressBarCallback, TensorboardCallback, EvalCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from sb3_contrib import RecurrentPPO, TQC, QRDQN, MaskablePPO, TRPO, ARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9b02a",
   "metadata": {},
   "source": [
    "# 2 - Cria funções de encoding/decoding em json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76215066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):            \n",
    "            return obj.tolist()\n",
    "        \n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "identifier ='Handling_manual'\n",
    "actions =[]\n",
    "message = ''\n",
    "\n",
    "\n",
    "def encode_json(identifier, actions):\n",
    "    data = {}\n",
    "    data['identifier'] = identifier\n",
    "    data['actions'] = actions\n",
    "    json_data = json.dumps(data, cls=NumpyEncoder)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9599a2",
   "metadata": {},
   "source": [
    "# 3 - Define função de criação do agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38498f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrillEnv(Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def _reset(self):\n",
    "        \n",
    "        self.reward = 0\n",
    "        self.action_name = ''\n",
    "        self.ConveyorFinish = False\n",
    "        self.GripperClose = False\n",
    "        self.Picked = False\n",
    "        self.GantryYposition = 0\n",
    "        self.GantryYatDestination = False\n",
    "        self.GantryZPosition = 0\n",
    "        self.GantryZatDestination = False\n",
    "        self.GantryYdriving = False\n",
    "        self.GantryZdriving = False\n",
    "        self.BoxSensor = False\n",
    "        self.ChangeCycle = False\n",
    "        actions = []\n",
    "        self.number_steps = 0\n",
    "        self.rewards = []\n",
    "        self.buffer_size = tamanho_buffer\n",
    "        self.state_buffer = np.zeros((self.buffer_size * 6))\n",
    "        self.rewards.append(0) \n",
    "        \n",
    "    def _obs(self):\n",
    "        obs= self.state_buffer                     \n",
    "        return obs\n",
    "        \n",
    "\n",
    "    def setprint(self, print):\n",
    "        self.print = print \n",
    "    def setprint2(self, print):\n",
    "        self.print2 = print\n",
    "    \n",
    "    def __init__(self):     \n",
    "        super(DrillEnv, self).__init__()\n",
    "        self.reset()\n",
    "\n",
    "        self.print = False\n",
    "        self.print2 = False\n",
    "        self.action_space = Discrete(11)\n",
    "        self.observation_space = Box(low=0, high=1, shape=(1,self.buffer_size * 6), dtype=np.float32)\n",
    "        \n",
    "    def buffer(self, ConveyorFinish, Picked, GantryYposition, \n",
    "                   GantryZPosition, GripperClose, ChangeCycle):\n",
    "\n",
    "        self.state_buffer = np.roll(self.state_buffer,6)         \n",
    "        self.state_buffer[0] = ConveyorFinish \n",
    "        self.state_buffer[1] = Picked \n",
    "        self.state_buffer[2] = GantryYposition\n",
    "        self.state_buffer[3] = GantryZPosition \n",
    "        self.state_buffer[4] = GripperClose \n",
    "        self.state_buffer[5] = ChangeCycle\n",
    "        \n",
    "    def step(self, action):\n",
    "        actions = action\n",
    "        \n",
    "        self.number_steps += 1                 \n",
    "        mensagem = encode_json('Handling_manual',  [actions])        \n",
    "        ws.send(mensagem)\n",
    "\n",
    "        data =''\n",
    "        station_identifier = ''\n",
    "        while  station_identifier != 'Handling':\n",
    "            try:\n",
    "                data = json.loads(ws.recv())                \n",
    "                station_identifier = (data['identifier'])\n",
    "            except:\n",
    "                return\n",
    "        \n",
    "        self.reward = int(data['reward'])\n",
    "         \n",
    "        if bool(data['done']) == False:\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "               \n",
    "        self.ConveyorFinish = bool(data['states'][0])\n",
    "        self.Picked = bool(data['states'][1])\n",
    "        self.GantryYposition = float(data['states'][2])\n",
    "        self.GantryZPosition = float(data['states'][3])\n",
    "        self.GripperClose = bool(data['states'][4])\n",
    "        self.ChangeCycle = bool(data['states'][5])\n",
    "                \n",
    "        self.buffer(self.ConveyorFinish, self.Picked, self.GantryYposition,self.GantryZPosition, self.GripperClose, self.ChangeCycle) \n",
    "        info = {}\n",
    "        obs=self._obs()\n",
    "        \n",
    "        if self.print:  \n",
    "            print((self.number_steps),'recompensa: ',(self.reward),'|  acao --> ', actions,'|  done: ', done, '| Estado: ',self.state_buffer[0:6])\n",
    "\n",
    "        # Return step information\n",
    "        return obs, self.reward, done, info\n",
    "    \n",
    "    def render(self , mode):\n",
    "        pass\n",
    "    def reset(self):        \n",
    "        self._reset()\n",
    "        return self._obs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c5551",
   "metadata": {},
   "source": [
    "# 4 - Cria o modelo em Aprendizagem por Reforço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84eeed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "algoritmo = A2C\n",
    "nome_do_arqu\n",
    "tamanho_buffer = 3 # tamanho do buffer aplicado ao algoritmo\n",
    "funcao_ativacao = Tanh #ex: Tanh, ELU, ReLU, Sigmoid, Softmax\n",
    "rede_neuronal=[dict(pi=[60,60], vf=[60,60])] # define número de nurónios na rede neuronal de política e rede neuronal de valor\n",
    "tipo_camadas = 'MlpPolicy' # define o tipo de camadas do modelo !!---(Para modelo em PPORecurrent, usar MlpLstmPolicy)---!!\n",
    "fator_desconto= 0.95 #fator de desconto para aprendizagem por reforço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c516b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (shared_net): Sequential()\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=18, out_features=60, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=18, out_features=60, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=60, out_features=11, bias=True)\n",
      "  (value_net): Linear(in_features=60, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "save_path = os.path.join('Training_Handling','Model_saves',f\"\"\"{algoritmo}_B{tamanho_buffer}\"\"\")\n",
    "policy_kwargs = dict(activation_fn=funcao_ativacao,net_arch=rede_neuronal)\n",
    "log_path = os.path.join('Training_Handling','Logs',f\"\"\"{algoritmo}_B{tamanho_buffer}\"\"\")\n",
    "\n",
    "env=DrillEnv()\n",
    "env.setprint(False)\n",
    "env=DummyVecEnv([lambda: env])\n",
    "env.reset()\n",
    "\n",
    "model=algoritmo(tipo_camadas,env,verbose=1,\n",
    "        gamma=fator_desconto, gae_lambda=0.95,\n",
    "        seed=9,policy_kwargs=policy_kwargs, \n",
    "        tensorboard_log=log_path)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=10000,\n",
    "  save_path=save_path ,\n",
    "  name_prefix=f\"\"\"{algoritmo}_B{tamanho_buffer}\"\"\")\n",
    "\n",
    "eval_callback = EvalCallback(env, n_eval_episodes=3,\n",
    "                             best_model_save_path=os.path.join('Training_Handling','Model_saves',f\"\"\"{algoritmo}_B{tamanho_buffer}_backup\"\"\",'Best_model'),\n",
    "                             log_path=log_path, eval_freq=4096,\n",
    "                             deterministic=False, render=False)\n",
    "callback = CallbackList([checkpoint_callback, ProgressBarCallback(), TensorboardCallback(), eval_callback])\n",
    "print(model.policy)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0702db40",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 123] A sintaxe do nome do arquivo, do nome do diretório ou do rótulo do volume está incorreta: \"Training_Handling\\\\Logs\\\\<class 'stable_baselines3.a2c.a2c.A2C'>_B3\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m,log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,callback\u001b[38;5;241m=\u001b[39mcallback)\n",
      "File \u001b[1;32mC:\\Python\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:203\u001b[0m, in \u001b[0;36mA2C.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m: A2CSelf,\n\u001b[0;32m    191\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    201\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m A2CSelf:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:246\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m: OnPolicyAlgorithmSelf,\n\u001b[0;32m    233\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    243\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OnPolicyAlgorithmSelf:\n\u001b[0;32m    244\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 246\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n",
      "File \u001b[1;32mC:\\Python\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:502\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, log_path, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# Configure logger's outputs if no logger was passed\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_logger:\n\u001b[1;32m--> 502\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure_logger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# Create eval callback if needed\u001b[39;00m\n\u001b[0;32m    505\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_callback(callback, eval_env, eval_freq, n_eval_episodes, log_path, progress_bar)\n",
      "File \u001b[1;32mC:\\Python\\lib\\site-packages\\stable_baselines3\\common\\utils.py:210\u001b[0m, in \u001b[0;36mconfigure_logger\u001b[1;34m(verbose, tensorboard_log, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m verbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    209\u001b[0m     format_strings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_strings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\lib\\site-packages\\stable_baselines3\\common\\logger.py:642\u001b[0m, in \u001b[0;36mconfigure\u001b[1;34m(folder, format_strings)\u001b[0m\n\u001b[0;32m    640\u001b[0m     folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tempfile\u001b[38;5;241m.\u001b[39mgettempdir(), datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSB3-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS-\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(folder, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m--> 642\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    644\u001b[0m log_suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_strings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python\\lib\\os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python\\lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] A sintaxe do nome do arquivo, do nome do diretório ou do rótulo do volume está incorreta: \"Training_Handling\\\\Logs\\\\<class 'stable_baselines3.a2c.a2c.A2C'>_B3\""
     ]
    }
   ],
   "source": [
    "ws = websocket.WebSocket()\n",
    "\n",
    "ws.connect(\"ws://127.0.0.1:12000\")\n",
    "time.sleep(1)\n",
    "\n",
    "for i in range(1):\n",
    "    model.learn(total_timesteps=1000000,log_interval=200,callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Avaliar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a610aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del modelTRPO\n",
    "# del model\n",
    "save_path = os.path.join('Dissertacao','Training_Handling','Model_saves','PPO_B03')\n",
    "#save_path = os.path.join('Training_Handling','Model_saves','modelTRPO+11')\n",
    "policy_kwargs = dict(activation_fn=th.nn.Tanh)\n",
    "#policy_kwargs = dict(activation_fn=th.nn.Tanh,\n",
    "#                     net_arch=[dict(pi=[6,60,30,60,9], vf=[6,60,30,60,9])])\n",
    "#log_path = os.path.join('Training_Handling','Logs','modelTRPO+11')\n",
    "env=DrillEnv()\n",
    "env.setprint(True)\n",
    "env=DummyVecEnv([lambda: env])\n",
    "env.reset()\n",
    "\n",
    "modelPPO=PPO('MlpPolicy',env,verbose=1,\n",
    "             #n_steps=5000,\n",
    "             gamma=0.95, gae_lambda=0.90,\n",
    "             seed=9,\n",
    "             policy_kwargs=policy_kwargs)\n",
    "model= modelPPO.load(save_path, print_system_info=True)\n",
    "\n",
    "ws = websocket.WebSocket()\n",
    "ws.connect(\"ws://127.0.0.1:12000\")\n",
    "time.sleep(1)\n",
    "mensagem = encode_json('CanConveyor_automatico', [])\n",
    "ws.send(mensagem)\n",
    "mensagem = encode_json('BoxConveyor_automatico', [])\n",
    "time.sleep(1)\n",
    "ws.send(mensagem)\n",
    "obs = env.reset()\n",
    "while True: \n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render() \n",
    "    if done: \n",
    "        obs = env.reset()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e1641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
